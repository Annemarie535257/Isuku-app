{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Isuku Chatbot - Llama Model Training\n",
        "\n",
        "This notebook trains a Llama-based chatbot model for the Isuku waste management system using the provided Q&A dataset.\n",
        "\n",
        "## Dataset Overview\n",
        "- **Total Rows**: 900\n",
        "- **Languages**: English, Kinyarwanda, French\n",
        "- **Intents**: waste_sorting, pickup_schedule, payment, education\n",
        "- **Split**: 70% Training, 20% Validation, 10% Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch accelerate datasets pandas openpyxl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: transformers in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (4.57.5)\n",
            "Requirement already satisfied: torch in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (2.8.0)\n",
            "Requirement already satisfied: accelerate in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (1.10.1)\n",
            "Requirement already satisfied: datasets in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (4.4.2)\n",
            "Requirement already satisfied: pandas in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (2.3.3)\n",
            "Requirement already satisfied: openpyxl in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (3.1.5)\n",
            "Requirement already satisfied: peft in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (0.17.1)\n",
            "Requirement already satisfied: bitsandbytes in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (0.42.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: psutil in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from accelerate) (7.2.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: et-xmlfile in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from requests->transformers) (2.6.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
            "Using cached scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
            "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install scikit-learn transformers torch accelerate datasets pandas openpyxl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/huberttuyishime/Documents/Isuku-app/venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)  # Should show the venv path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/huberttuyishime/Documents/Isuku-app/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape: (900, 5)\n",
            "\n",
            "Column Names: ['ID', 'Intent', 'Language', 'Question', 'Answer']\n",
            "\n",
            "First few rows:\n",
            "   ID           Intent     Language                          Question  \\\n",
            "0   1    waste_sorting      English           How do I sort my waste?   \n",
            "1   2    waste_sorting  Kinyarwanda              Nashyira he imyanda?   \n",
            "2   3    waste_sorting       French       Comment trier les déchets ?   \n",
            "3   4  pickup_schedule      English  When will my waste be collected?   \n",
            "4   5  pickup_schedule  Kinyarwanda          Imyanda ikusanywa ryari?   \n",
            "\n",
            "                                              Answer  \n",
            "0  You should separate waste into organic, recycl...  \n",
            "1  Ugomba gutandukanya imyanda mu byiciro: ibora,...  \n",
            "2  Vous devez séparer les déchets en déchets orga...  \n",
            "3  Check your pickup schedule in the app or wait ...  \n",
            "4  Reba gahunda yo kuyikusanya muri porogaramu cy...  \n",
            "\n",
            "============================================================\n",
            "Dataset Statistics\n",
            "============================================================\n",
            "Total rows: 900\n",
            "Languages: ['English' 'Kinyarwanda' 'French']\n",
            "Intents: ['waste_sorting' 'pickup_schedule' 'payment' 'education']\n",
            "\n",
            "Language distribution:\n",
            "Language\n",
            "English        300\n",
            "Kinyarwanda    300\n",
            "French         300\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Intent distribution:\n",
            "Intent\n",
            "waste_sorting      225\n",
            "pickup_schedule    225\n",
            "payment            225\n",
            "education          225\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_excel('Dataset/isuku_chatbot_dataset_300_QA.xlsx')\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumn Names:\", df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Dataset Statistics\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Languages: {df['Language'].unique()}\")\n",
        "print(f\"Intents: {df['Intent'].unique()}\")\n",
        "print(f\"\\nLanguage distribution:\\n{df['Language'].value_counts()}\")\n",
        "print(f\"\\nIntent distribution:\\n{df['Intent'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing and Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Q&A pairs: 12 (from 900 total rows)\n",
            "\n",
            "Sample formatted prompts:\n",
            "\n",
            "--- Sample 1 (English, waste_sorting) ---\n",
            "### Instruction:\n",
            "How do I sort my waste?\n",
            "\n",
            "### Response:\n",
            "You should separate waste into organic, recyclable, and non-recyclable items.\n",
            "\n",
            "--- Sample 2 (Kinyarwanda, waste_sorting) ---\n",
            "### Instruction:\n",
            "Nashyira he imyanda?\n",
            "\n",
            "### Response:\n",
            "Ugomba gutandukanya imyanda mu byiciro: ibora, ishobora gukoreshwa ukundi, n’itagira icyo imaze.\n",
            "\n",
            "--- Sample 3 (French, waste_sorting) ---\n",
            "### Instruction:\n",
            "Comment trier les déchets ?\n",
            "\n",
            "### Response:\n",
            "Vous devez séparer les déchets en déchets organiques, recyclables et non recyclables.\n"
          ]
        }
      ],
      "source": [
        "# Remove duplicates to get unique Q&A pairs\n",
        "df_unique = df.drop_duplicates(subset=['Question', 'Answer', 'Language', 'Intent'])\n",
        "print(f\"Unique Q&A pairs: {len(df_unique)} (from {len(df)} total rows)\")\n",
        "\n",
        "# Format data for chatbot training\n",
        "# Create a prompt template for instruction-following format\n",
        "def format_prompt(row):\n",
        "    \"\"\"Format Q&A pair into instruction-following format\"\"\"\n",
        "    prompt = f\"### Instruction:\\n{row['Question']}\\n\\n### Response:\\n{row['Answer']}\"\n",
        "    return prompt\n",
        "\n",
        "df_unique['formatted_text'] = df_unique.apply(format_prompt, axis=1)\n",
        "\n",
        "# Display sample formatted text\n",
        "print(\"\\nSample formatted prompts:\")\n",
        "for i in range(min(3, len(df_unique))):\n",
        "    print(f\"\\n--- Sample {i+1} ({df_unique.iloc[i]['Language']}, {df_unique.iloc[i]['Intent']}) ---\")\n",
        "    print(df_unique.iloc[i]['formatted_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Validation/Test Split (70/20/10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum samples per class: 1\n",
            "Warning: Some classes have < 2 samples. Using random split without stratification.\n",
            "Warning: Some classes in temp split have < 2 samples. Using random split.\n",
            "Training set: 8 samples (66.7%)\n",
            "Validation set: 2 samples (16.7%)\n",
            "Test set: 2 samples (16.7%)\n",
            "\n",
            "Training set distribution:\n",
            "  Languages: {'French': 3, 'Kinyarwanda': 3, 'English': 2}\n",
            "  Intents: {'pickup_schedule': 3, 'waste_sorting': 2, 'payment': 2, 'education': 1}\n",
            "\n",
            "Validation set distribution:\n",
            "  Languages: {'Kinyarwanda': 1, 'English': 1}\n",
            "  Intents: {'education': 1, 'waste_sorting': 1}\n",
            "\n",
            "Test set distribution:\n",
            "  Languages: {'English': 1, 'French': 1}\n",
            "  Intents: {'education': 1, 'payment': 1}\n"
          ]
        }
      ],
      "source": [
        "# Create a combined label for stratification\n",
        "df_unique['stratify_label'] = df_unique['Language'] + '_' + df_unique['Intent']\n",
        "\n",
        "# Check if we can use stratification (need at least 2 samples per class)\n",
        "stratify_counts = df_unique['stratify_label'].value_counts()\n",
        "min_samples = stratify_counts.min()\n",
        "print(f\"Minimum samples per class: {min_samples}\")\n",
        "\n",
        "# First split: 70% train, 30% temp (for val + test)\n",
        "if min_samples >= 2:\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df_unique,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=df_unique['stratify_label']  # Stratify to maintain distribution\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: Some classes have < 2 samples. Using random split without stratification.\")\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df_unique,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "# Second split: 20% val, 10% test (from the 30% temp)\n",
        "# temp_df is 30%, so 20/30 = 0.667 for validation, 10/30 = 0.333 for test\n",
        "temp_stratify_counts = temp_df['stratify_label'].value_counts()\n",
        "temp_min_samples = temp_stratify_counts.min()\n",
        "\n",
        "if temp_min_samples >= 2:\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=0.333,  # 10% of total / 30% of total = 0.333\n",
        "        random_state=42,\n",
        "        stratify=temp_df['stratify_label']\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: Some classes in temp split have < 2 samples. Using random split.\")\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=0.333,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"Training set: {len(train_df)} samples ({len(train_df)/len(df_unique)*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df_unique)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_unique)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nTraining set distribution:\")\n",
        "print(f\"  Languages: {train_df['Language'].value_counts().to_dict()}\")\n",
        "print(f\"  Intents: {train_df['Intent'].value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\nValidation set distribution:\")\n",
        "print(f\"  Languages: {val_df['Language'].value_counts().to_dict()}\")\n",
        "print(f\"  Intents: {val_df['Intent'].value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\nTest set distribution:\")\n",
        "print(f\"  Languages: {test_df['Language'].value_counts().to_dict()}\")\n",
        "print(f\"  Intents: {test_df['Intent'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Llama Model and Tokenizer\n",
        "\n",
        "**Note**: For this example, we'll use a smaller Llama model. You may need to:\n",
        "- Use Hugging Face authentication token for Llama models\n",
        "- Adjust model name based on available resources\n",
        "- Consider using quantized models (4-bit/8-bit) for memory efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Model parameters: 1100.05M\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "# Using a smaller Llama model variant for training\n",
        "# You can change this to \"meta-llama/Llama-2-7b-chat-hf\" or other variants\n",
        "# Note: You may need Hugging Face authentication for Llama models\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small model for demonstration\n",
        "# Alternative options:\n",
        "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # Requires HF token\n",
        "# MODEL_NAME = \"microsoft/phi-2\"  # Alternative small model\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Configure quantization for memory efficiency (optional)\n",
        "use_quantization = True  # Set to False if you have enough GPU memory\n",
        "\n",
        "if use_quantization and torch.cuda.is_available():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "# Move model to device if not using device_map\n",
        "if not torch.cuda.is_available():\n",
        "    model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing training set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8/8 [00:00<00:00, 99.30 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2/2 [00:00<00:00, 260.28 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2/2 [00:00<00:00, 455.38 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training samples: 8\n",
            "Validation samples: 2\n",
            "Test samples: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the formatted text\"\"\"\n",
        "    # Tokenize with truncation and padding\n",
        "    # Reduced max_length from 512 to 256 to save memory on MPS\n",
        "    tokenized = tokenizer(\n",
        "        examples['formatted_text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=256,  # Reduced from 512 to save memory\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # For causal LM, labels are the same as input_ids\n",
        "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
        "    return tokenized\n",
        "\n",
        "# Convert to HuggingFace datasets\n",
        "train_dataset = Dataset.from_pandas(train_df[['formatted_text']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['formatted_text']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['formatted_text']])\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing training set...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['formatted_text']\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation set...\")\n",
        "val_dataset = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['formatted_text']\n",
        ")\n",
        "\n",
        "print(\"Tokenizing test set...\")\n",
        "test_dataset = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['formatted_text']\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configure LoRA for Efficient Fine-tuning\n",
        "\n",
        "Using LoRA (Low-Rank Adaptation) to reduce memory requirements and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
            "\n",
            "LoRA configuration applied successfully!\n"
          ]
        }
      ],
      "source": [
        "# Prepare model for LoRA training\n",
        "if use_quantization:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # LoRA alpha\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Adjust based on model architecture\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"\\nLoRA configuration applied successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS detected. Memory limit disabled (may cause system issues if memory is exhausted).\n",
            "If you encounter memory issues, consider reducing batch size further or using CPU.\n",
            "Training arguments configured!\n",
            "Output directory: ./models/isuku_chatbot_llama/checkpoints\n",
            "Training epochs: 3\n",
            "Batch size: 1\n",
            "Learning rate: 0.0002\n"
          ]
        }
      ],
      "source": [
        "# Create models directory for saving the trained model\n",
        "MODEL_SAVE_DIR = \"./models/isuku_chatbot_llama\"\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Training arguments\n",
        "output_dir = \"./models/isuku_chatbot_llama/checkpoints\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Configure MPS memory settings for Apple Silicon (if using MPS)\n",
        "if torch.backends.mps.is_available():\n",
        "    import os\n",
        "    # Set MPS memory limit to avoid out-of-memory errors\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
        "    print(\"MPS detected. Memory limit disabled (may cause system issues if memory is exhausted).\")\n",
        "    print(\"If you encounter memory issues, consider reducing batch size further or using CPU.\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,  # Adjust based on your needs\n",
        "    per_device_train_batch_size=1,  # Reduced from 2 to 1 to save memory\n",
        "    per_device_eval_batch_size=1,  # Reduced from 2 to 1 to save memory\n",
        "    gradient_accumulation_steps=8,  # Increased from 4 to 8 to maintain effective batch size = 1 * 8 = 8\n",
        "    warmup_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available (not for MPS)\n",
        "    bf16=False,  # Disable bf16 for MPS\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy for newer transformers\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",  # Set to \"tensorboard\" if you want to use TensorBoard\n",
        "    push_to_hub=False,\n",
        "    dataloader_pin_memory=False,  # Disable pin memory for MPS to save memory\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Initialize Trainer and Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Force CPU to avoid MPS memory issues\n",
        "import torch\n",
        "torch.backends.mps.is_available = lambda: False\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer initialized. Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 1:34:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.800055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.797337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.791894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "\n",
            "Saving model and tokenizer...\n",
            "Model saved to: ./models/isuku_chatbot_llama\n",
            "Training metrics saved!\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized. Starting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Save the final model and tokenizer\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "trainer.save_model(MODEL_SAVE_DIR)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
        "print(f\"Model saved to: {MODEL_SAVE_DIR}\")\n",
        "\n",
        "# Save training metrics\n",
        "import json\n",
        "train_metrics = trainer.state.log_history\n",
        "with open(f\"{MODEL_SAVE_DIR}/training_metrics.json\", \"w\") as f:\n",
        "    json.dump(train_metrics, f, indent=2)\n",
        "print(\"Training metrics saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation and Testing\n",
        "\n",
        "Evaluate the trained model on the test set and generate sample responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model for evaluation...\n",
            "Model loaded successfully for evaluation!\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model for evaluation\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"Loading saved model for evaluation...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load the fine-tuned LoRA weights\n",
        "trained_model = PeftModel.from_pretrained(base_model, MODEL_SAVE_DIR)\n",
        "trained_model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_DIR)\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded successfully for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on test set...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:36]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Set Results:\n",
            "  eval_loss: 3.2555\n",
            "  eval_runtime: 77.7701\n",
            "  eval_samples_per_second: 0.0260\n",
            "  eval_steps_per_second: 0.0260\n",
            "  epoch: 3.0000\n",
            "\n",
            "Test results saved to: ./models/isuku_chatbot_llama/test_results.json\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"\\nTest Set Results:\")\n",
        "for key, value in test_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save test results\n",
        "with open(f\"{MODEL_SAVE_DIR}/test_results.json\", \"w\") as f:\n",
        "    json.dump(test_results, f, indent=2)\n",
        "print(f\"\\nTest results saved to: {MODEL_SAVE_DIR}/test_results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response generation function created!\n"
          ]
        }
      ],
      "source": [
        "# Function to generate response from the model\n",
        "def generate_response(model, tokenizer, question, max_length=256, temperature=0.7):\n",
        "    \"\"\"Generate a response to a given question\"\"\"\n",
        "    # Format the prompt\n",
        "    prompt = f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    \n",
        "    # Move to device\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the response part\n",
        "    if \"### Response:\" in full_response:\n",
        "        response = full_response.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        response = full_response[len(prompt):].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"Response generation function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Test Model with Sample Questions\n",
        "\n",
        "Test the trained model with sample questions from the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model with sample questions from test set...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Language: French | Intent: payment\n",
            "\n",
            "Question: Comment payer le service de collecte ?\n",
            "\n",
            "Expected Answer: Le paiement se fait via mobile money dans l’application.\n",
            "\n",
            "Generated Answer: Nous avons répondu à votre demande de service de collecte le jour de votre réception.\n",
            "\n",
            "Pour plus d'informations, veuillez contacter notre équipe de réception.\n",
            "\n",
            "Nous espérons voir votre réception dans les plus brefs délais.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name]\n",
            "\n",
            "[Your Title]\n",
            "\n",
            "[Your Company Name]\n",
            "\n",
            "[Your Company Address]\n",
            "\n",
            "[Your City, State ZIP Code]\n",
            "\n",
            "[Your Phone Number]\n",
            "\n",
            "[Your Email Address]\n",
            "\n",
            "[Your Website]\n",
            "\n",
            "[Date]\n",
            "\n",
            "============================================================\n",
            "Language: English | Intent: education\n",
            "\n",
            "Question: Why is waste sorting important?\n",
            "\n",
            "Expected Answer: It helps recycling, reduces pollution, and protects the environment.\n",
            "\n",
            "Generated Answer: Waste sorting is important because it helps to reduce the amount of waste that ends up in landfills and improves the overall sustainability of our environment. Landfills are not only a source of pollution but also a significant contributor to climate change. By sorting and recycling waste, we can reduce the amount of waste that ends up in landfills and divert it from the landfill. It also helps to reduce the amount of energy needed to treat and dispose of waste. Overall, waste sorting is essential for a more sustainable and environmentally friendly future.\n",
            "\n",
            "============================================================\n",
            "Sample testing completed!\n",
            "\n",
            "Test samples and results saved to: ./models/isuku_chatbot_llama/test_samples_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Test with sample questions from test set\n",
        "print(\"Testing model with sample questions from test set...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Select a few diverse samples from test set\n",
        "test_samples = test_df.sample(min(5, len(test_df)), random_state=42)\n",
        "\n",
        "results = []\n",
        "for idx, row in test_samples.iterrows():\n",
        "    question = row['Question']\n",
        "    expected_answer = row['Answer']\n",
        "    language = row['Language']\n",
        "    intent = row['Intent']\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Language: {language} | Intent: {intent}\")\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(f\"\\nExpected Answer: {expected_answer}\")\n",
        "    \n",
        "    # Generate response\n",
        "    generated_answer = generate_response(trained_model, eval_tokenizer, question)\n",
        "    print(f\"\\nGenerated Answer: {generated_answer}\")\n",
        "    \n",
        "    results.append({\n",
        "        'question': question,\n",
        "        'expected': expected_answer,\n",
        "        'generated': generated_answer,\n",
        "        'language': language,\n",
        "        'intent': intent\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Sample testing completed!\")\n",
        "\n",
        "# Save test samples and results\n",
        "test_samples_df = pd.DataFrame(results)\n",
        "test_samples_df.to_csv(f\"{MODEL_SAVE_DIR}/test_samples_results.csv\", index=False)\n",
        "print(f\"\\nTest samples and results saved to: {MODEL_SAVE_DIR}/test_samples_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Interactive Testing\n",
        "\n",
        "Test the model with your own custom questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with custom question:\n",
            "Question: How do I schedule a waste pickup?\n",
            "\n",
            "Generating response...\n",
            "\n",
            "Response: Sure! To schedule a waste pickup, please call the city's waste management hotline at 718-638-5575. The hotline is available 24/7 and can handle your request. Once you have scheduled your pickup, you will receive a notification via email or text message. If you have any questions or concerns, you can contact the hotline at any time.\n"
          ]
        }
      ],
      "source": [
        "# Interactive testing - modify the question below to test your model\n",
        "custom_question = \"How do I schedule a waste pickup?\"\n",
        "\n",
        "print(\"Testing with custom question:\")\n",
        "print(f\"Question: {custom_question}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "response = generate_response(trained_model, eval_tokenizer, custom_question)\n",
        "print(f\"\\nResponse: {response}\")\n",
        "\n",
        "# You can test more questions by running this cell again with different questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Model Summary\n",
        "\n",
        "Summary of the trained model and saved files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL TRAINING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model Name: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Model Save Directory: ./models/isuku_chatbot_llama\n",
            "\n",
            "Dataset Statistics:\n",
            "  Total unique Q&A pairs: 12\n",
            "  Training samples: 8\n",
            "  Validation samples: 2\n",
            "  Test samples: 2\n",
            "\n",
            "Saved Files in ./models/isuku_chatbot_llama:\n",
            "  - adapter_model.safetensors (17.21 MB)\n",
            "  - tokenizer_config.json (0.00 MB)\n",
            "  - special_tokens_map.json (0.00 MB)\n",
            "  - test_samples_results.csv (0.00 MB)\n",
            "  - tokenizer.json (3.45 MB)\n",
            "  - checkpoints/ (directory)\n",
            "  - README.md (0.00 MB)\n",
            "  - training_args.bin (0.01 MB)\n",
            "  - training_metrics.json (0.00 MB)\n",
            "  - adapter_config.json (0.00 MB)\n",
            "  - chat_template.jinja (0.00 MB)\n",
            "  - test_results.json (0.00 MB)\n",
            "\n",
            "============================================================\n",
            "Training and evaluation completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Print model summary and saved files\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel Name: {MODEL_NAME}\")\n",
        "print(f\"Model Save Directory: {MODEL_SAVE_DIR}\")\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Total unique Q&A pairs: {len(df_unique)}\")\n",
        "print(f\"  Training samples: {len(train_df)}\")\n",
        "print(f\"  Validation samples: {len(val_df)}\")\n",
        "print(f\"  Test samples: {len(test_df)}\")\n",
        "\n",
        "print(f\"\\nSaved Files in {MODEL_SAVE_DIR}:\")\n",
        "if os.path.exists(MODEL_SAVE_DIR):\n",
        "    for file in os.listdir(MODEL_SAVE_DIR):\n",
        "        file_path = os.path.join(MODEL_SAVE_DIR, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"  - {file} ({size:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"  - {file}/ (directory)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training and evaluation completed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
